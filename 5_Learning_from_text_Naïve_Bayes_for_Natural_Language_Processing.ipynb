{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5 Learning from text - Naïve Bayes for Natural Language Processing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atlas-github/20190731StarMediaGroup/blob/master/5_Learning_from_text_Na%C3%AFve_Bayes_for_Natural_Language_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdABiWZmBH0Z",
        "colab_type": "text"
      },
      "source": [
        "_This is one of the 100+ free recipes of the [IPython Cookbook, Second Edition](https://ipython-books.github.io/), by [Cyrille Rossant](http://cyrille.rossant.net/), a guide to numerical computing and data science in the Jupyter Notebook. The ebook and printed book are available for purchase at [Packt Publishing](https://www.packtpub.com/big-data-and-business-intelligence/ipython-interactive-computing-and-visualization-cookbook-second-e)._\n",
        "\n",
        "+ ▶  _[Text on GitHub](https://github.com/ipython-books/cookbook-2nd) with a [CC-BY-NC-ND license](https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode)_\n",
        "+ ▶  _[Code on GitHub](https://github.com/ipython-books/cookbook-2nd-code) with a [MIT license](https://opensource.org/licenses/MIT)_\n",
        "+ ▶  _[Go to Chapter 8 : Machine Learning](https://ipython-books.github.io/chapter-8-machine-learning/)_\n",
        "+ ▶  _[Get the Jupyter notebook](https://github.com/ipython-books/cookbook-2nd-code)_\n",
        "\n",
        "In this recipe, we show how to handle text data with scikit-learn. Working with text requires careful preprocessing and feature extraction. It is also quite common to deal with highly sparse matrices.\n",
        "\n",
        "We will learn to recognize whether a comment posted during a public discussion is considered insulting to one of the participants. We will use a labeled dataset from Impermium, released during a Kaggle competition (see http://www.kaggle.com/c/detecting-insults-in-social-commentary)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voxcCaDjBKlz",
        "colab_type": "text"
      },
      "source": [
        "# How to do it..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hs3T1k-eBMoT",
        "colab_type": "text"
      },
      "source": [
        "1. Let's import our libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6V7z6cGBDED",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import sklearn.model_selection as ms\n",
        "import sklearn.feature_extraction.text as text\n",
        "import sklearn.naive_bayes as nb\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "le14s6CqBOoc",
        "colab_type": "text"
      },
      "source": [
        "2. Let's open the CSV file with pandas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boGdgCuzBPy0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/ipython-books/cookbook-2nd-data/master/troll.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeBflbq1BW6Q",
        "colab_type": "text"
      },
      "source": [
        "3. Each row is a comment. We will consider two columns: whether the comment is insulting (1) or not (0) and the unicode-encoded contents of the comment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pb_tNaWHBUYt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df[['Insult', 'Comment']].tail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a36JYLAjBcfN",
        "colab_type": "text"
      },
      "source": [
        "4. Now, we are going to define the feature matrix $X$ and the labels $y$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taeTsjGgBZhK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = df['Insult']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqg58AYkBfbT",
        "colab_type": "text"
      },
      "source": [
        "Obtaining the feature matrix from the text is not trivial. scikit-learn can only work with numerical matrices. So how do we convert text into a matrix of numbers? A classical solution is to first extract a __vocabulary__, a list of words used throughout the corpus. Then, we count, for each sample, the frequency of each word. We end up with a __sparse matrix__, a huge matrix containing mostly zeros. Here, we do this in two lines. We will give more details in _How it works...._\n",
        "\n",
        "The general rule here is that whenever one of our features is categorical (that is, the presence of a word, a color belonging to a fixed set of __n__ colors, and so on), we should vectorize it by considering one binary feature per item in the class. For example, instead of a feature color being __red__, __green__, or __blue__, we should consider three binary features __color_red__, __color_green__, and __color_blue__. We give further references in the _There's more..._ section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaDD7J-ABeHc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf = text.TfidfVectorizer()\n",
        "X = tf.fit_transform(df['Comment'])\n",
        "print(X.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dptG_7gnBlyp",
        "colab_type": "text"
      },
      "source": [
        "5. There are 3947 comments and 16469 different words. Let's estimate the sparsity of this feature matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SHI6bdmBlVO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p = 100 * X.nnz / float(X.shape[0] * X.shape[1])\n",
        "print(f\"Each sample has ~{p:.2f}% non-zero features.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8qsnDqRBpRx",
        "colab_type": "text"
      },
      "source": [
        "6. Now, we are going to train a classifier as usual. We first split the data into a train and test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC0epRx1BomR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train, X_test, y_train, y_test) = \\\n",
        "    ms.train_test_split(X, y, test_size=.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4vPqEZvBtq5",
        "colab_type": "text"
      },
      "source": [
        "7. We use a __Bernoulli Naive Bayes classifier__ with a grid search on the $α$ parameter:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqXc5wzyBsWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bnb = ms.GridSearchCV(\n",
        "    nb.BernoulliNB(),\n",
        "    param_grid={'alpha': np.logspace(-2., 2., 50)}, cv=5)\n",
        "bnb.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_OWNyNtB7TJ",
        "colab_type": "text"
      },
      "source": [
        "8. Let's check the performance of this classifier on the test dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MqOxAnwBwVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bnb.score(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mPx9X__B-hE",
        "colab_type": "text"
      },
      "source": [
        "9. Let's take a look at the words corresponding to the largest coefficients (the words we find frequently in insulting comments):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiluxioZB9P8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We first get the words corresponding to each feature\n",
        "names = np.asarray(tf.get_feature_names())\n",
        "# Next, we display the 50 words with the largest\n",
        "# coefficients.\n",
        "print(','.join(names[np.argsort(\n",
        "    bnb.best_estimator_.coef_[0, :])[::-1][:50]]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gk75WwSCCZZ",
        "colab_type": "text"
      },
      "source": [
        "10. Finally, let's test our estimator on a few test sentences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aw3FfWWmCB6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(bnb.predict(tf.transform([\n",
        "    \"I totally agree with you.\",\n",
        "    \"You are so stupid.\"\n",
        "])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIF8P8ylCHlQ",
        "colab_type": "text"
      },
      "source": [
        "# How it works..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsu0gBrNCIEX",
        "colab_type": "text"
      },
      "source": [
        "scikit-learn implements several utility functions to obtain a sparse feature matrix from text data. A __vectorizer__ such as __CountVectorizer()__ extracts a vocabulary from a corpus (__fit()__) and constructs a sparse representation of the corpus based on this vocabulary (__transform()__). Each sample is represented by the vocabulary's word frequencies. The trained instance also contains attributes and methods to map feature indices to the corresponding words (__get_feature_names()__) and conversely (__vocabulary___).\n",
        "\n",
        "N-grams can also be extracted: those are pairs or tuples of words occurring successively (__ngram_range keyword__).\n",
        "\n",
        "The frequency of the words can be weighted in different ways. Here, we have used __tf-idf__, or __term frequency-inverse document frequency__. This quantity reflects how important a word is to a corpus. Frequent words in comments have a high weight except if they appear in most comments (which means that they are common terms, for example, \"the\" and \"and\" would be filtered out using this technique).\n",
        "\n",
        "Naive Bayes algorithms are Bayesian methods based on the naive assumption of independence between the features. This strong assumption drastically simplifies the computations and leads to very fast yet decent classifiers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAGDOGxRDuty",
        "colab_type": "text"
      },
      "source": [
        "# Another Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kC1wtgdVJBeY",
        "colab_type": "text"
      },
      "source": [
        "##Description\n",
        "\n",
        "This dataset contains headlines, URLs, and categories for 422,937 news stories collected by a web aggregator between March 10th, 2014 and August 10th, 2014.\n",
        "\n",
        "News categories included in this dataset include business; science and technology; entertainment; and health. Different news articles that refer to the same news item (e.g., several articles about recently released employment statistics) are also categorized together.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoIUZ6xzJI5b",
        "colab_type": "text"
      },
      "source": [
        "##Content\n",
        "\n",
        "The columns included in this dataset are:\n",
        "\n",
        "* ID : the numeric ID of the article\n",
        "* TITLE : the headline of the article\n",
        "* URL : the URL of the article\n",
        "* PUBLISHER : the publisher of the article\n",
        "* CATEGORY : the category of the news item; one of: -- b : business -- t : science and technology -- e : entertainment -- m : health\n",
        "* STORY : alphanumeric ID of the news story that the article discusses\n",
        "* HOSTNAME : hostname where the article was posted\n",
        "* TIMESTAMP : approximate timestamp of the article's publication, given in Unix time (seconds since midnight on Jan 1, 1970)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9FhRL7HJi4m",
        "colab_type": "text"
      },
      "source": [
        "## Acknowledgments\n",
        "\n",
        "This dataset comes from the UCI Machine Learning Repository. Any publications that use this data should cite the repository as follows:\n",
        "\n",
        "Lichman, M. (2013). [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml). Irvine, CA: University of California, School of Information and Computer Science.\n",
        "\n",
        "This specific dataset can be found in the UCI ML Repository at [this URL](http://archive.ics.uci.edu/ml/datasets/News+Aggregator)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHMAVaZ0JwPJ",
        "colab_type": "text"
      },
      "source": [
        "## Inspiration\n",
        "\n",
        "What kinds of questions can we explore using this dataset? Here are a few possibilities:\n",
        "\n",
        "* can we predict the category (business, entertainment, etc.) of a news article given only its headline?\n",
        "* can we predict the specific story that a news article refers to, given only its headline?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_hXmx0rIGWH",
        "colab_type": "text"
      },
      "source": [
        "###Step 1: Load data\n",
        "\n",
        "1. Records in the file are comma delimited;\n",
        "2. Column titles are included in the text file;\n",
        "3. Load the data into a Pandas data frame;\n",
        "4. View unique values for the category `column` for later transformation to discrete numerical values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8VgzN5rDuh6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "news_df = pd.read_csv(\"https://raw.githubusercontent.com/GJBroughton/UCI-News-Aggregator-Classifier/master/data/uci-news-aggregator.csv\", sep = \",\")\n",
        "news_df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uszGc3bXIs4Y",
        "colab_type": "text"
      },
      "source": [
        "### Step 2. Preprocess data\n",
        "1. Transform categories into discrete numerical values;\n",
        "2. Transform all words to lowercase;\n",
        "3. Remove all punctuations.\n",
        "\n",
        "NOTE: \n",
        "* 1: business\n",
        "* 2: technology\n",
        "* 3: entertainment\n",
        "* 4: health"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfqeHEEcIxrQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "\n",
        "news_df['CATEGORY'] = news_df.CATEGORY.map({ 'b': 1, 't': 2, 'e': 3, 'm': 4 })\n",
        "news_df['TITLE'] = news_df.TITLE.map(\n",
        "    lambda x: x.lower().translate(str.maketrans('','', string.punctuation))\n",
        ")\n",
        "\n",
        "news_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duEs_G9gI3cH",
        "colab_type": "text"
      },
      "source": [
        "###Step 3: Split into train and test data sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdS7ZjuaI71F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    news_df['TITLE'], \n",
        "    news_df['CATEGORY'], \n",
        "    random_state = 1\n",
        ")\n",
        "\n",
        "print(\"Training dataset: \", X_train.shape[0])\n",
        "print(\"Test dataset: \", X_test.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUx2xUgdJ8Qr",
        "colab_type": "text"
      },
      "source": [
        "###Step 4: Extract features\n",
        "\n",
        "Apply bag of words processing to the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wd41NnvfKAy6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count_vector = CountVectorizer(stop_words = 'english')\n",
        "training_data = count_vector.fit_transform(X_train)\n",
        "testing_data = count_vector.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmVOR1lFKH-7",
        "colab_type": "text"
      },
      "source": [
        "###Step 5: Train Multinomial Naive Bayes classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSORE2suKNOX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "naive_bayes = MultinomialNB()\n",
        "naive_bayes.fit(training_data, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1m0JnSToKM9K",
        "colab_type": "text"
      },
      "source": [
        "###Step 6: Generate predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrmT309OKUoM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = naive_bayes.predict(testing_data)\n",
        "predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogslpyrrkBlU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGX-_10Df9sI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test2 = X_test.reset_index(drop = True)\n",
        "print(X_test2.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dl-faq9jMkv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "type(X_test2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUEFOSHsM1wS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "type(predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdzRMw1CNkkF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_predictions = pd.Series(predictions)\n",
        "type(new_predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1Z6Yg00PPhZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(new_predictions.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJL6sTwfWnZ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "joined = pd.concat([X_test2, new_predictions], axis = 1) \n",
        "print(joined.head(10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fgFr2qIMvtC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "joined.to_csv('joined.csv')\n",
        "files.download('joined.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGkzQR7UKSjq",
        "colab_type": "text"
      },
      "source": [
        "###Step 7: Evaluate model performance\n",
        "\n",
        "This is a multi-class classification. So, for these evaulation scores, explicitly specify average = weighted"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AUYvs0IKbGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "\n",
        "print(\"Accuracy score: \", accuracy_score(y_test, predictions))\n",
        "print(\"Recall score: \", recall_score(y_test, predictions, average = 'weighted'))\n",
        "print(\"Precision score: \", precision_score(y_test, predictions, average = 'weighted'))\n",
        "print(\"F1 score: \", f1_score(y_test, predictions, average = 'weighted'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5wwthpAlj0r",
        "colab_type": "text"
      },
      "source": [
        "### Step 8: Comparing old records with predicted records"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcHou4jYljVG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "comparison = pd.merge(joined, news_df, on = \"TITLE\", how = \"inner\")\n",
        "comparison.loc[(comparison[0])!=comparison[\"CATEGORY\"]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJpfM_tcBytj",
        "colab_type": "text"
      },
      "source": [
        "###OPTIONAL: Use one of the functions [here] to predict for new headlines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_r5j42UCN1P",
        "colab_type": "text"
      },
      "source": [
        "# There's more..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBee3blWCP1a",
        "colab_type": "text"
      },
      "source": [
        "Here are a few references:\n",
        "\n",
        "+ Text feature extraction in scikit-learn's documentation, available at http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
        "+ Term frequency-inverse document-frequency on Wikipedia, available at https://en.wikipedia.org/wiki/tf-idf\n",
        "+ Vectorizer in scikit-learn's documentation, available at http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html\n",
        "+ Naive Bayes classifier on Wikipedia, at https://en.wikipedia.org/wiki/Naive_Bayes_classifier\n",
        "+ Naive Bayes in scikit-learn's documentation, available at http://scikit-learn.org/stable/modules/naive_bayes.html\n",
        "+ Document classification example in scikit-learn's documentation, at http://scikit-learn.org/stable/datasets/twenty_newsgroups.html\n",
        "+ News article classification using Naive Bayes at https://www.kaggle.com/cnokello/news-article-classification-using-naive-bayes/data\n",
        "\n",
        "Here are other natural language processing libraries in Python:\n",
        "\n",
        "+ NLTK available at http://www.nltk.org\n",
        "+ spaCy available at https://spacy.io/\n",
        "+ textacy available at http://textacy.readthedocs.io/en/stable/"
      ]
    }
  ]
}