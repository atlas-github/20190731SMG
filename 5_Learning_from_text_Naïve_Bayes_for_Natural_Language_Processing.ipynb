{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5 Learning from text - Naïve Bayes for Natural Language Processing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atlas-github/20190731StarMediaGroup/blob/master/5_Learning_from_text_Na%C3%AFve_Bayes_for_Natural_Language_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdABiWZmBH0Z",
        "colab_type": "text"
      },
      "source": [
        "_This is one of the 100+ free recipes of the [IPython Cookbook, Second Edition](https://ipython-books.github.io/), by [Cyrille Rossant](http://cyrille.rossant.net/), a guide to numerical computing and data science in the Jupyter Notebook. The ebook and printed book are available for purchase at [Packt Publishing](https://www.packtpub.com/big-data-and-business-intelligence/ipython-interactive-computing-and-visualization-cookbook-second-e)._\n",
        "\n",
        "+ ▶  _[Text on GitHub](https://github.com/ipython-books/cookbook-2nd) with a [CC-BY-NC-ND license](https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode)_\n",
        "+ ▶  _[Code on GitHub](https://github.com/ipython-books/cookbook-2nd-code) with a [MIT license](https://opensource.org/licenses/MIT)_\n",
        "+ ▶  _[Go to Chapter 8 : Machine Learning](https://ipython-books.github.io/chapter-8-machine-learning/)_\n",
        "+ ▶  _[Get the Jupyter notebook](https://github.com/ipython-books/cookbook-2nd-code)_\n",
        "\n",
        "In this recipe, we show how to handle text data with scikit-learn. Working with text requires careful preprocessing and feature extraction. It is also quite common to deal with highly sparse matrices.\n",
        "\n",
        "We will learn to recognize whether a comment posted during a public discussion is considered insulting to one of the participants. We will use a labeled dataset from Impermium, released during a Kaggle competition (see http://www.kaggle.com/c/detecting-insults-in-social-commentary)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voxcCaDjBKlz",
        "colab_type": "text"
      },
      "source": [
        "# How to do it..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hs3T1k-eBMoT",
        "colab_type": "text"
      },
      "source": [
        "1. Let's import our libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6V7z6cGBDED",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import sklearn.model_selection as ms\n",
        "import sklearn.feature_extraction.text as text\n",
        "import sklearn.naive_bayes as nb\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "le14s6CqBOoc",
        "colab_type": "text"
      },
      "source": [
        "2. Let's open the CSV file with pandas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boGdgCuzBPy0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('https://github.com/ipython-books/'\n",
        "                 'cookbook-2nd-data/blob/master/'\n",
        "                 'troll.csv?raw=true')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeBflbq1BW6Q",
        "colab_type": "text"
      },
      "source": [
        "3. Each row is a comment. We will consider two columns: whether the comment is insulting (1) or not (0) and the unicode-encoded contents of the comment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pb_tNaWHBUYt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df[['Insult', 'Comment']].tail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a36JYLAjBcfN",
        "colab_type": "text"
      },
      "source": [
        "4. Now, we are going to define the feature matrix $X$ and the labels $y$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taeTsjGgBZhK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = df['Insult']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqg58AYkBfbT",
        "colab_type": "text"
      },
      "source": [
        "Obtaining the feature matrix from the text is not trivial. scikit-learn can only work with numerical matrices. So how do we convert text into a matrix of numbers? A classical solution is to first extract a __vocabulary__, a list of words used throughout the corpus. Then, we count, for each sample, the frequency of each word. We end up with a __sparse matrix__, a huge matrix containing mostly zeros. Here, we do this in two lines. We will give more details in _How it works...._\n",
        "\n",
        "The general rule here is that whenever one of our features is categorical (that is, the presence of a word, a color belonging to a fixed set of __n__ colors, and so on), we should vectorize it by considering one binary feature per item in the class. For example, instead of a feature color being __red__, __green__, or __blue__, we should consider three binary features __color_red__, __color_green__, and __color_blue__. We give further references in the _There's more..._ section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaDD7J-ABeHc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf = text.TfidfVectorizer()\n",
        "X = tf.fit_transform(df['Comment'])\n",
        "print(X.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dptG_7gnBlyp",
        "colab_type": "text"
      },
      "source": [
        "5. There are 3947 comments and 16469 different words. Let's estimate the sparsity of this feature matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SHI6bdmBlVO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p = 100 * X.nnz / float(X.shape[0] * X.shape[1])\n",
        "print(f\"Each sample has ~{p:.2f}% non-zero features.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8qsnDqRBpRx",
        "colab_type": "text"
      },
      "source": [
        "6. Now, we are going to train a classifier as usual. We first split the data into a train and test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC0epRx1BomR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train, X_test, y_train, y_test) = \\\n",
        "    ms.train_test_split(X, y, test_size=.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4vPqEZvBtq5",
        "colab_type": "text"
      },
      "source": [
        "7. We use a __Bernoulli Naive Bayes classifier__ with a grid search on the $α$ parameter:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqXc5wzyBsWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bnb = ms.GridSearchCV(\n",
        "    nb.BernoulliNB(),\n",
        "    param_grid={'alpha': np.logspace(-2., 2., 50)}, cv=5)\n",
        "bnb.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_OWNyNtB7TJ",
        "colab_type": "text"
      },
      "source": [
        "8. Let's check the performance of this classifier on the test dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MqOxAnwBwVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bnb.score(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mPx9X__B-hE",
        "colab_type": "text"
      },
      "source": [
        "9. Let's take a look at the words corresponding to the largest coefficients (the words we find frequently in insulting comments):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiluxioZB9P8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We first get the words corresponding to each feature\n",
        "names = np.asarray(tf.get_feature_names())\n",
        "# Next, we display the 50 words with the largest\n",
        "# coefficients.\n",
        "print(','.join(names[np.argsort(\n",
        "    bnb.best_estimator_.coef_[0, :])[::-1][:50]]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gk75WwSCCZZ",
        "colab_type": "text"
      },
      "source": [
        "10. Finally, let's test our estimator on a few test sentences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aw3FfWWmCB6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(bnb.predict(tf.transform([\n",
        "    \"I totally agree with you.\",\n",
        "    \"You are so stupid.\"\n",
        "])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIF8P8ylCHlQ",
        "colab_type": "text"
      },
      "source": [
        "# How it works..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsu0gBrNCIEX",
        "colab_type": "text"
      },
      "source": [
        "scikit-learn implements several utility functions to obtain a sparse feature matrix from text data. A __vectorizer__ such as __CountVectorizer()__ extracts a vocabulary from a corpus (__fit()__) and constructs a sparse representation of the corpus based on this vocabulary (__transform()__). Each sample is represented by the vocabulary's word frequencies. The trained instance also contains attributes and methods to map feature indices to the corresponding words (__get_feature_names()__) and conversely (__vocabulary___).\n",
        "\n",
        "N-grams can also be extracted: those are pairs or tuples of words occurring successively (__ngram_range keyword__).\n",
        "\n",
        "The frequency of the words can be weighted in different ways. Here, we have used __tf-idf__, or __term frequency-inverse document frequency__. This quantity reflects how important a word is to a corpus. Frequent words in comments have a high weight except if they appear in most comments (which means that they are common terms, for example, \"the\" and \"and\" would be filtered out using this technique).\n",
        "\n",
        "Naive Bayes algorithms are Bayesian methods based on the naive assumption of independence between the features. This strong assumption drastically simplifies the computations and leads to very fast yet decent classifiers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_r5j42UCN1P",
        "colab_type": "text"
      },
      "source": [
        "# There's more..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBee3blWCP1a",
        "colab_type": "text"
      },
      "source": [
        "Here are a few references:\n",
        "\n",
        "+ Text feature extraction in scikit-learn's documentation, available at http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
        "+ Term frequency-inverse document-frequency on Wikipedia, available at https://en.wikipedia.org/wiki/tf-idf\n",
        "+ Vectorizer in scikit-learn's documentation, available at http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html\n",
        "+ Naive Bayes classifier on Wikipedia, at https://en.wikipedia.org/wiki/Naive_Bayes_classifier\n",
        "+ Naive Bayes in scikit-learn's documentation, available at http://scikit-learn.org/stable/modules/naive_bayes.html\n",
        "+ Document classification example in scikit-learn's documentation, at http://scikit-learn.org/stable/datasets/twenty_newsgroups.html\n",
        "\n",
        "Here are other natural language processing libraries in Python:\n",
        "\n",
        "+ NLTK available at http://www.nltk.org\n",
        "+ spaCy available at https://spacy.io/\n",
        "+ textacy available at http://textacy.readthedocs.io/en/stable/"
      ]
    }
  ]
}