{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3B Estimating the correlation between two variables with a contingency table and a chi-squared test.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atlas-github/20190731StarMediaGroup/blob/master/3B_Estimating_the_correlation_between_two_variables_with_a_contingency_table_and_a_chi_squared_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Urs9kM2SvFk5",
        "colab_type": "text"
      },
      "source": [
        "_This is one of the 100+ free recipes of the [IPython Cookbook, Second Edition](https://ipython-books.github.io/), by [Cyrille Rossant](http://cyrille.rossant.net/), a guide to numerical computing and data science in the Jupyter Notebook. The ebook and printed book are available for purchase at [Packt Publishing](https://www.packtpub.com/big-data-and-business-intelligence/ipython-interactive-computing-and-visualization-cookbook-second-e)._\n",
        "\n",
        "+ ▶  _[Text on GitHub](https://github.com/ipython-books/cookbook-2nd) with a [CC-BY-NC-ND license](https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode)_\n",
        "+ ▶  _[Code on GitHub](https://github.com/ipython-books/cookbook-2nd-code) with a [MIT license](https://opensource.org/licenses/MIT)_\n",
        "+ ▶  _[Go to Chapter 7 : Statistical Data Analysis](https://ipython-books.github.io/chapter-7-statistical-data-analysis/)_\n",
        "+ ▶  _[Get the Jupyter notebook](https://github.com/ipython-books/cookbook-2nd-code)_\n",
        "\n",
        "Whereas univariate methods deal with single-variable observations, multivariate methods consider observations with several features. Multivariate datasets allow the study of _relations_ between variables, more particularly their correlation, or lack thereof (that is, independence)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6K7yu4se5hq",
        "colab_type": "text"
      },
      "source": [
        "# A simple example first"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq_Ww8BypuQU",
        "colab_type": "text"
      },
      "source": [
        "We'll be looking at data from the census in 1994. Specifically, we are interested in the relationship between 'sex' and 'hours-per-week' worked. Click [here](https://archive.ics.uci.edu/ml/datasets/Census+Income) for the documentation and citation of the data. First let's get the assumptions out of the way:\n",
        "\n",
        "There must be different participants in each group with no participant being in more than one group. In our case, each individual can only have one 'sex' and can not be in multiple workhour categories.\n",
        "Random samples from the population. In our case, the census is a good representation of the population."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oV89wc5VlWGX",
        "colab_type": "text"
      },
      "source": [
        "##Data exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKTsAOrNpryU",
        "colab_type": "text"
      },
      "source": [
        "For the sake of this example, we'll convert the numerical column 'hours-per-week' into a categorical column using pandas. Then we'll assign 'sex' and 'hours_per_week_categories' to a new dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISRY3yi-oFZT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8PuNNTTlWQ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cols = ['age', 'workclass', 'fnlwg', 'education', 'education-num', 'marital-status','occupation','relationship', 'race','sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
        "data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data', names = cols)\n",
        "data.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QNUwgeKoXFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create a column for work hour categories.\n",
        "def process_hours(df):\n",
        "    cut_points = [0,9,19,29,39,49,1000]\n",
        "    label_names = [\"0-9\",\"10-19\",\"20-29\",\"30-39\",\"40-49\",\"50+\"]\n",
        "    df[\"hours_per_week_categories\"] = pd.cut(df[\"hours-per-week\"],\n",
        "                                             cut_points,labels=label_names)\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8Ab8kF_ochC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = process_hours(data)\n",
        "workhour_by_sex = data[['sex', 'hours_per_week_categories']]\n",
        "workhour_by_sex.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOyaRcDHoh4U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "workhour_by_sex['sex'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWaOYvhUoh7C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "workhour_by_sex['hours_per_week_categories'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cuBctxiozIW",
        "colab_type": "text"
      },
      "source": [
        "## The Null and Alternate Hypothesis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5SWUW1Ko3lW",
        "colab_type": "text"
      },
      "source": [
        "Recall that we are interested in knowing if there is a relationship between 'sex' and 'hours_per_week_categories'. In order to do so, we would have to use the Chi-squared test. But first, let's state our null hypothesis and the alternative hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7sIiRy-pIah",
        "colab_type": "text"
      },
      "source": [
        "$H_0$: There is no statistically significant relationship between sex and the number of hours per week worked.\n",
        "\n",
        "$H_1$: There is a statistically significant relationship between sex and the number of hours per week worked."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwCtPWyGpkyW",
        "colab_type": "text"
      },
      "source": [
        "## Construct the Contingency Table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WSWcr8Up6Rj",
        "colab_type": "text"
      },
      "source": [
        "The next step is to format the data into a frequency count table. This is called a **Contingency Table**, we can accomplish this by using the pd.crosstab() function in pandas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WE5goWzEpbOC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "contingency_table = pd.crosstab(\n",
        "    workhour_by_sex['sex'],\n",
        "    workhour_by_sex['hours_per_week_categories'],\n",
        "    margins = True\n",
        ")\n",
        "contingency_table"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlNydhd6qF4T",
        "colab_type": "text"
      },
      "source": [
        "Each cell in this table represents a frequency count. For example, the intersection of the 'Male' row and the '10-19' column of the table would represent the number of males who works 10-19 hours per week from our sample data set. The intersection of the 'All' row and the '50+' column would represent the total number of people who works 50+ hours a week."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM9mcfSLqTiC",
        "colab_type": "text"
      },
      "source": [
        "##Visualize the Contingency Table with a Stacked  Bar Chart"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aczP6L8pY32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Assigns the frequency values\n",
        "malecount = contingency_table.iloc[0][0:6].values\n",
        "femalecount = contingency_table.iloc[1][0:6].values\n",
        "\n",
        "#Plots the bar chart\n",
        "fig = plt.figure(figsize=(10, 5))\n",
        "sns.set(font_scale=1.8)\n",
        "categories = [\"0-9\",\"10-19\",\"20-29\",\"30-39\",\"40-49\",\"50+\"]\n",
        "p1 = plt.bar(categories, malecount, 0.55, color='#d62728')\n",
        "p2 = plt.bar(categories, femalecount, 0.55, bottom=malecount)\n",
        "plt.legend((p2[0], p1[0]), ('Male', 'Female'))\n",
        "plt.xlabel('Hours per Week Worked')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsungS6_qlGi",
        "colab_type": "text"
      },
      "source": [
        "The chart above visualizes our sample data from the census. If there is truly no relationship between sex and the number of hours per week worked. Then the data would show an even ratio split between 'Male' and 'Female' for each time category. For example, if 5% of the females worked 50+ hours, we would expect the same percentage for males who worked 50+ hours."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VJPskLfrAN8",
        "colab_type": "text"
      },
      "source": [
        "##The Chi-Squared Test for Independence - Calculation with Numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrBPM4GXramF",
        "colab_type": "text"
      },
      "source": [
        "In order to determine whether we accept or reject the null hypothesis. We have to compute p-value similar to the other statistical tests. For testing with two categorical variables, we will use the Chi-squared test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPTDI-BzrhLh",
        "colab_type": "text"
      },
      "source": [
        "$\\chi^2 = \\frac {(observed - expected)^2}{expected}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtuYUSiMr4fh",
        "colab_type": "text"
      },
      "source": [
        "Where $\\chi^2$ is the test statistic, $observed$ are values we have in the contingency table, $expected$ are values we would expect assuming the null hypothesis is true. Theoretically speaking, if all the expected values are equal to the observed values, then the $\\chi^2$ statistic will be $0$. As a result, the null hypothesis will be retained.\n",
        "\n",
        "First, let's put the observed values into a one dimensional array, reading the contingency table from left to right then top to bottom."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KMxN73nsFWH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f_obs = np.append(contingency_table.iloc[0][0:6].values, contingency_table.iloc[1][0:6].values)\n",
        "f_obs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohTEUquCsWZQ",
        "colab_type": "text"
      },
      "source": [
        "Next, we need to calculate the expected values. The expected values assume that null hypothesis is true. We would need to calculate values if there is an equal percentage of males and females for each category. For example, this is how we would calculate the expected value for the top left cell:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfubqQ_nsb09",
        "colab_type": "text"
      },
      "source": [
        "Expected number of $Females$ in the $'0 - 9'$ category = $\\frac{(Total number of Females * Number of People in the '0 - 9' category)}{Total number of people}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkCAm8NwsFTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "row_sums = contingency_table.iloc[0:2,6].values\n",
        "row_sums"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-hA2WBao2v3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "col_sums = contingency_table.iloc[2,[4, 1, 5, 3, 2, 0]].values\n",
        "col_sums"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0hL0qQKpFw8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total = contingency_table.loc['All', 'All']\n",
        "\n",
        "f_expected = []\n",
        "for j in range(2):\n",
        "    for i in col_sums:\n",
        "        f_expected.append(i*row_sums[j]/total)\n",
        "f_expected"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TPWpi6FtVkw",
        "colab_type": "text"
      },
      "source": [
        "Now that we have all our observed and expected values, we can just plug everything into the Chi-squared test formula."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NugJq9KjtW0s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chi_squared_statistic = ((f_obs - f_expected)**2/f_expected).sum()\n",
        "print('Chi-squared Statistic: {}'.format(chi_squared_statistic))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieoXwwjUv7p5",
        "colab_type": "text"
      },
      "source": [
        "## Degrees of Freedom"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0lbI5qhtWlZ",
        "colab_type": "text"
      },
      "source": [
        "We would have to calculate the degrees of freedom before we can determine the p-value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv21lzmhv-p1",
        "colab_type": "text"
      },
      "source": [
        "$DoF = (Numberofrows - 1)*(Numberofcolumns - 1)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfCxmsdXwDRf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dof = (len(row_sums)-1)*(len(col_sums)-1)\n",
        "print(\"Degrees of Freedom: {}\".format(dof))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcDVQuxNwccg",
        "colab_type": "text"
      },
      "source": [
        "Now we are ready to look into the Chi-squared distribution [table](http://www.itl.nist.gov/div898/handbook/eda/section3/eda3674.htm). The cut off for a p-value of $0.05$ was $11.070$. Our $\\chi^2$ statistic was so large that the p-value is approximately zero. So we have evidence against the null hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWMf8o6Uwrd0",
        "colab_type": "text"
      },
      "source": [
        "##The Chi-Squared Test for Independence - Using Scipy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUxMdnqgw00n",
        "colab_type": "text"
      },
      "source": [
        "Now that we've gone through all the calculations, it is time to look for shortcuts. Scipy has a function that plugs in all the values for us. Click [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html) for the documentation.\n",
        "\n",
        "All we need to do is format the observed values into a two-dimensional array and plug it into the function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AV3Jo8-Pv-2H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f_obs = np.array([contingency_table.iloc[0][0:6].values,\n",
        "                  contingency_table.iloc[1][0:6].values])\n",
        "f_obs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "py5Wajm_xDLl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import stats\n",
        "stats.chi2_contingency(f_obs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yQ_efJyxa_V",
        "colab_type": "text"
      },
      "source": [
        "The results were exactly the same as our calculations with Numpy. The $\\chi^2 = ~2287$, p-value $= ~0$ and degrees of freedom $= 5$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPk6rYGpxkIf",
        "colab_type": "text"
      },
      "source": [
        "##Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P0eIYxgxm0J",
        "colab_type": "text"
      },
      "source": [
        "With a p-value $< 0.05$ , we can reject the null hypothesis. There is definitely some sort of relationship between 'sex' and the 'hours-per-week' column. We don't know what this relationship is, but we do know that these two variables are not independent of each other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3eyHzECvJKv",
        "colab_type": "text"
      },
      "source": [
        "# How to do it..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3znocU4yeOA",
        "colab_type": "text"
      },
      "source": [
        "In this recipe, we will take a look at a tennis dataset. Following a frequentist approach, we will estimate the correlation between the number of [aces](https://en.wikipedia.org/wiki/Ace_(tennis)) and the proportion of points won by a tennis player."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vq-goSKKvSuH",
        "colab_type": "text"
      },
      "source": [
        "1. Let's import NumPy, pandas, SciPy.stats, and matplotlib:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5PLmPk9u2k9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as st\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQdE13envVyg",
        "colab_type": "text"
      },
      "source": [
        "2. We download and load the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJz-cS6gvUaB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "player = 'Roger Federer'\n",
        "df = pd.read_csv('https://github.com/ipython-books/'\n",
        "                 'cookbook-2nd-data/blob/master/'\n",
        "                 'federer.csv?raw=true',\n",
        "                 parse_dates=['start date'],\n",
        "                 dayfirst=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4yRei3wvZir",
        "colab_type": "text"
      },
      "source": [
        "3. Each row corresponds to a match, and the 70 columns contain many player characteristics during that match:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwK2D1-ZvXjP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f\"Number of columns: {len(df.columns)}\")\n",
        "df[df.columns[:4]].tail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3fIn1a4vdTA",
        "colab_type": "text"
      },
      "source": [
        "4. Here, we only look at the proportion of points won, and the (relative) number of aces:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6b0aW_gvbGN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "npoints = df['player1 total points total']\n",
        "points = df['player1 total points won'] / npoints\n",
        "aces = df['player1 aces'] / npoints\n",
        "fig, ax = plt.subplots(1, 1)\n",
        "ax.plot(points, aces, '.')\n",
        "ax.set_xlabel('% of points won')\n",
        "ax.set_ylabel('% of aces')\n",
        "ax.set_xlim(0., 1.)\n",
        "ax.set_ylim(0.)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUV4Ytigvg7J",
        "colab_type": "text"
      },
      "source": [
        "If the two variables were independent, we would not see any trend in the cloud of points. On this plot, it is a bit hard to tell. Let's use pandas to compute a coefficient correlation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPwpxZMxvkEC",
        "colab_type": "text"
      },
      "source": [
        "5. For simplicity, we create a new __DataFrame__ object with only these fields. We also remove the rows where one field is missing (using __dropna()__):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dipe6SU8vez8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_bis = pd.DataFrame({'points': points,\n",
        "                       'aces': aces}).dropna()\n",
        "df_bis.tail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00estcxuvnoy",
        "colab_type": "text"
      },
      "source": [
        "6. Let's compute the Pearson's correlation coefficient between the relative number of aces in the match, and the number of points won:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "As69xGQMviak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_bis.corr()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "radx9CfmvshA",
        "colab_type": "text"
      },
      "source": [
        "A correlation of ~0.26 seems to indicate a positive correlation between our two variables. In other words, the more aces in a match, the more points the player wins (which is not very surprising!)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PHqQ_R8vuuQ",
        "colab_type": "text"
      },
      "source": [
        "7. Now, to determine if there is a statistically significant correlation between the variables, we use a __chi-squared test__ of the independence of variables in a __contingency table__. \n",
        "8.  First, we binarize our variables. Here, the value corresponding to the number of aces is __True__ if the player is serving more aces than usual in a match, and __False__ otherwise:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-GmVOzQvp6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_bis['result'] = (df_bis['points'] >\n",
        "                    df_bis['points'].median())\n",
        "df_bis['manyaces'] = (df_bis['aces'] >\n",
        "                      df_bis['aces'].median())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TAOk4SPvyk8",
        "colab_type": "text"
      },
      "source": [
        "9. Then, we create a contingency table, with the frequencies of all four possibilities (True and True, True and False, and so on):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcOk7HIBvxAr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.crosstab(df_bis['result'], df_bis['manyaces'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFHRaYp_v2cx",
        "colab_type": "text"
      },
      "source": [
        "10. Finally, we compute the chi-squared test statistic and the associated p-value. The null hypothesis is the independence between the variables. SciPy implements this test in __scipy.stats.chi2_contingency()__, which returns several objects. We're interested in the second result, which is the p-value:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fveR65HBv00Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "st.chi2_contingency(_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1K6oQ-4cv4bR",
        "colab_type": "text"
      },
      "source": [
        "The p-value is much lower than 0.05, so we reject the null hypothesis and conclude that there is a statistically significant correlation between the proportion of aces and the proportion of points won in a match in this dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1j3jSQShv7Ro",
        "colab_type": "text"
      },
      "source": [
        "Correlation does not imply causation. Here, it is likely that external factors influence both variables. See https://en.wikipedia.org/wiki/Correlation_does_not_imply_causation for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14wFEpxczu3l",
        "colab_type": "text"
      },
      "source": [
        "# Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AN5YoApRzxba",
        "colab_type": "text"
      },
      "source": [
        "1. Find the $\\chi^2$ value for features **income** and **education** from the **data** table (it's the first data set in this notebook) using the [.chi2_congtingency()](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html) function.\n",
        "2. Find the correlation between **education**, **education-num**, **capital-gain**, **hours-per-week**, **income**, **fnlwg**, and **age** using the [.corr()](https://www.geeksforgeeks.org/python-pandas-dataframe-corr/) function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBYLqld5zxfz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Start with this code\n",
        "data2 = data[['education', 'education-num', 'capital-gain', 'hours-per-week', 'income', 'fnlwg', 'age']]\n",
        "data2.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9gQcO5j4KUE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##INSERT CODE HERE: Build a crosstab table\n",
        "###Share your code on https://codeshare.io/axEYAR\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74VOur7l4YhT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##INSERT CODE HERE: Get the chi-square value by using the .chi2_contingency function\n",
        "###Share your code on https://codeshare.io/axEYAR\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdRijoqE59nN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##INSERT CODE HERE: find the correlation between education-num, capital-gain, hours-per-week, income, fnlwg, and age features using the .corr() function\n",
        "###Share your code on https://codeshare.io/axEYAR\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k5IXUUlv-sy",
        "colab_type": "text"
      },
      "source": [
        "# How it works..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5Y7oeexwAVO",
        "colab_type": "text"
      },
      "source": [
        "We give here a few details about the statistical concepts used in this recipe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV6wmlIMwC4J",
        "colab_type": "text"
      },
      "source": [
        "## Pearson's correlation coefficient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsNav2wFwFh_",
        "colab_type": "text"
      },
      "source": [
        "Pearson's correlation coefficient measures the linear correlation between two random variables, $X$ and $Y$. It is a normalized version of the covariance:\n",
        "\n",
        "$$ρ=\\frac{cov(X,Y)}{\\sqrt {var(X)var(Y)}} = \\frac{E((X−E(X))(Y−E(Y)))}{\\sqrt {var(X)var(Y)}}$$\n",
        "\n",
        "It can be estimated by substituting, in this formula, the expectancy with the sample mean, and the variance with the sample variance. More details about its inference can be found at https://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E577IUMawIXX",
        "colab_type": "text"
      },
      "source": [
        "## Contingency table and chi-squared test "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eW20rrBwKoV",
        "colab_type": "text"
      },
      "source": [
        "The contingency table contains the frequencies $O_{ij}$ of all combinations of outcomes, when there are multiple random variables that can take a finite number of values. Under the null hypothesis of independence, we can compute the _expected frequencies_ $E_{ij}$, based on the marginal sums (sums in each row). The chi-squared statistic, by definition, is:\n",
        "\n",
        "$$χ=∑_{i,j} \\frac{(O_{ij}−E_{ij})^2}{E_{ij}}$$\n",
        "\n",
        "When there are sufficiently many observations, this variable approximately follows a chi-squared distribution (the distribution of the sum of normal variables squared). Once we get the p-value, as explained in the _Getting started with statistical hypothesis testing – a simple z-test_ recipe, we can reject or accept the null hypothesis of independence. Then, we can conclude (or not) that there exists a significant correlation between the variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpTsMpecwMNq",
        "colab_type": "text"
      },
      "source": [
        "# There's more..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBhbNBqqwT-j",
        "colab_type": "text"
      },
      "source": [
        "There are many other sorts of chi-squared tests, that is, tests where the test statistic follows a chi-squared distribution. These tests are widely used for testing the goodness-of-fit of a distribution, or testing the independence of variables. More information can be found in the following pages:\n",
        "\n",
        "+ Chi2 test in SciPy documentation available at http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html\n",
        "+ Contingency table introduced at https://en.wikipedia.org/wiki/Contingency_table\n",
        "+ Chi-squared test introduced at https://en.wikipedia.org/wiki/Pearson's_chi-squared_test"
      ]
    }
  ]
}